{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>responce</th>\n",
       "      <th>question</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>603</td>\n",
       "      <td>@115904 We'll be sure to pass along your kind ...</td>\n",
       "      <td>@AmericanAir Erica on the lax team is amazing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>605</td>\n",
       "      <td>@115904 Our apologies for the delay in respond...</td>\n",
       "      <td>@AmericanAir Could you have someone on your la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>608</td>\n",
       "      <td>@115905 Aww, that's definitely a future pilot ...</td>\n",
       "      <td>Ben Tennyson and an American Airlines pilot. ðŸŽƒ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>612</td>\n",
       "      <td>@115906 We're sorry for your frustration.</td>\n",
       "      <td>@AmericanAir Right, but I earned those. I also...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>618</td>\n",
       "      <td>@115909 We're glad you got to kick back and en...</td>\n",
       "      <td>Thank you, @AmericanAir for playing #ThisIsUs ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           responce  \\\n",
       "0         603  @115904 We'll be sure to pass along your kind ...   \n",
       "1         605  @115904 Our apologies for the delay in respond...   \n",
       "2         608  @115905 Aww, that's definitely a future pilot ...   \n",
       "3         612          @115906 We're sorry for your frustration.   \n",
       "4         618  @115909 We're glad you got to kick back and en...   \n",
       "\n",
       "                                            question  \n",
       "0  @AmericanAir Erica on the lax team is amazing ...  \n",
       "1  @AmericanAir Could you have someone on your la...  \n",
       "2  Ben Tennyson and an American Airlines pilot. ðŸŽƒ...  \n",
       "3  @AmericanAir Right, but I earned those. I also...  \n",
       "4  Thank you, @AmericanAir for playing #ThisIsUs ...  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "rev = pd.read_csv(r\"question_responce.csv\")\n",
    "rev.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "stop_words=set(nltk.corpus.stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(headline):\n",
    "    le=WordNetLemmatizer()\n",
    "    word_tokens=word_tokenize(headline)\n",
    "    tokens=[le.lemmatize(w) for w in word_tokens if w not in stop_words and len(w)>3]\n",
    "    cleaned_text=\" \".join(tokens)\n",
    "    return cleaned_text\n",
    "rev['cleaned_text']=rev['question'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                           Erica team amazing give raise\n",
      "1            Could someone team available guide gate ASAP\n",
      "2       Tennyson American Airlines pilot trunkortreat ...\n",
      "3        Right earned also pas spouse need change program\n",
      "4       Thank  playing ThisIsUs great flight attendant...\n",
      "                              ...                        \n",
      "1847     nailed transatlantic WiFi service able join 1...\n",
      "1848                            Average price ticket 2500\n",
      "1849     Really annoyed month since damaged claim neve...\n",
      "1850     terrible service wait age trying call number ...\n",
      "1851     charge patron change flight every time airpor...\n",
      "Name: cleaned_text, Length: 1852, dtype: object\n"
     ]
    }
   ],
   "source": [
    "rev[\"cleaned_text\"] = rev[\"cleaned_text\"].str.replace('AmericanAir','')\n",
    "rev[\"cleaned_text\"] = rev[\"cleaned_text\"].str.replace('http','')\n",
    "print(rev.cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 412)\t0.5529514529306722\n",
      "  (0, 54)\t0.5939070649598848\n",
      "  (0, 879)\t0.5843963457214582\n",
      "  (1, 78)\t0.5225608089087591\n",
      "  (1, 407)\t0.29135401046590886\n",
      "  (1, 90)\t0.42532865299362604\n",
      "  (1, 826)\t0.3951006690273464\n",
      "  (1, 219)\t0.3644699121367777\n",
      "  (1, 879)\t0.4149647250090478\n",
      "  (2, 185)\t0.3876113606433001\n",
      "  (2, 56)\t0.33519824012199595\n",
      "  (2, 11)\t0.4396722834272028\n",
      "  (2, 431)\t0.4592973392656634\n",
      "  (2, 690)\t0.310682514221521\n",
      "  (2, 42)\t0.36472730223257405\n",
      "  (2, 55)\t0.321844963626745\n",
      "  (3, 718)\t0.426452435408527\n",
      "  (3, 154)\t0.3242530242433215\n",
      "  (3, 625)\t0.28355004236601444\n",
      "  (3, 671)\t0.4016683695772027\n",
      "  (3, 52)\t0.36034610238509834\n",
      "  (3, 293)\t0.4660683225748587\n",
      "  (3, 770)\t0.352407712950135\n",
      "  (4, 463)\t0.41539759593853104\n",
      "  (4, 97)\t0.40306907044082546\n",
      "  :\t:\n",
      "  (1849, 814)\t0.28733652195466486\n",
      "  (1849, 175)\t0.32202482108375513\n",
      "  (1849, 280)\t0.2975232847332916\n",
      "  (1849, 627)\t0.25783089707831724\n",
      "  (1849, 614)\t0.2975232847332916\n",
      "  (1849, 733)\t0.24923233474029236\n",
      "  (1849, 905)\t0.26649614912731695\n",
      "  (1849, 97)\t0.23277104759527792\n",
      "  (1850, 883)\t0.3445704769312068\n",
      "  (1850, 759)\t0.3023123126505597\n",
      "  (1850, 128)\t0.3044943679754405\n",
      "  (1850, 639)\t0.3168427106560821\n",
      "  (1850, 614)\t0.3226292963947118\n",
      "  (1850, 804)\t0.217532217191797\n",
      "  (1850, 48)\t0.3290600960810796\n",
      "  (1850, 420)\t0.39981585894151356\n",
      "  (1850, 957)\t0.3044943679754405\n",
      "  (1850, 926)\t0.29071001391643814\n",
      "  (1851, 180)\t0.5097650127608043\n",
      "  (1851, 158)\t0.42103126219964565\n",
      "  (1851, 318)\t0.4323912387381287\n",
      "  (1851, 900)\t0.2839732200479713\n",
      "  (1851, 44)\t0.35588084714844226\n",
      "  (1851, 373)\t0.1721733538385862\n",
      "  (1851, 154)\t0.3727928296447279\n"
     ]
    }
   ],
   "source": [
    "# Building a dictionary of responses\n",
    "\n",
    "vect =TfidfVectorizer(stop_words=stop_words,max_features=1000)\n",
    "vect_text=vect.fit_transform(rev['cleaned_text'])\n",
    "print(vect_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_model=LatentDirichletAllocation(n_components=20,learning_method='online',random_state=42,max_iter=1) \n",
    "lda_top=lda_model.fit_transform(vect_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0183066  0.0183066  0.0183066  0.01830662 0.65217451 0.0183066\n",
      " 0.0183066  0.0183066  0.0183066  0.0183066  0.0183066  0.0183066\n",
      " 0.0183066  0.0183066  0.0183066  0.0183066  0.0183066  0.0183066\n",
      " 0.0183066  0.0183066 ]\n"
     ]
    }
   ],
   "source": [
    "print(lda_top[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1: \n",
      "Topic  0 :  1.3815841480660318 %\n",
      "Topic  1 :  39.759770851935556 %\n",
      "Topic  2 :  1.3815841453207605 %\n",
      "Topic  3 :  1.3815841550868397 %\n",
      "Topic  4 :  1.3815841565226727 %\n",
      "Topic  5 :  1.381584145311549 %\n",
      "Topic  6 :  1.3815841477191166 %\n",
      "Topic  7 :  1.3815841939776825 %\n",
      "Topic  8 :  1.3815841461219234 %\n",
      "Topic  9 :  1.3815841452514457 %\n",
      "Topic  10 :  1.381584147758489 %\n",
      "Topic  11 :  1.3815841454612399 %\n",
      "Topic  12 :  1.3815841464331735 %\n",
      "Topic  13 :  1.3815841452863147 %\n",
      "Topic  14 :  1.3815841466234657 %\n",
      "Topic  15 :  1.3815841491606866 %\n",
      "Topic  16 :  1.3815841745813682 %\n",
      "Topic  17 :  35.371714417693795 %\n",
      "Topic  18 :  1.381584145212333 %\n",
      "Topic  19 :  1.3815841464755567 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Document 1: \")\n",
    "for i,topic in enumerate(lda_top[2]):\n",
    "  print(\"Topic \",i,\": \",topic*100,\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 0: \n",
      "checked luggage fantastic stay turkey kudos yall error lame standard \n",
      "Topic 1: \n",
      "co sent flight airport mile delayed american today time month \n",
      "Topic 2: \n",
      "much locator record transit screen mail route laptop economy basic \n",
      "Topic 3: \n",
      "flight thank great service co time please help gate home \n",
      "Topic 4: \n",
      "upgrade class check first lounge boarding co using people flight \n",
      "Topic 5: \n",
      "child nonsense drink young really disappointed airplane level economy parent \n",
      "Topic 6: \n",
      "good look flight start attendant made amazing thanks co worked \n",
      "Topic 7: \n",
      "flying never year co time refund awesome yesterday airline every \n",
      "Topic 8: \n",
      "price response suitcase detail frequent status flyer kept public telling \n",
      "Topic 9: \n",
      "hell entertainment purchased aircraft mile earned counting aadvantage statement card \n",
      "Topic 10: \n",
      "gate plane wifi delayed broken agent number late flight wish \n",
      "Topic 11: \n",
      "info fact showing idea meal literally calling upgrade list evening \n",
      "Topic 12: \n",
      "found lost switch dreamliner complete husband woman honesty help friday \n",
      "Topic 13: \n",
      "exactly experience along answer story likely thanks trip name feel \n",
      "Topic 14: \n",
      "ticket award online well paying flight point going awful charged \n",
      "Topic 15: \n",
      "seat flight like would waiting gate still follow passenger send \n",
      "Topic 16: \n",
      "co hour flight early morning delay time thanks thanksgiving landed \n",
      "Topic 17: \n",
      "pilot 21st step issue maintenance flight 2017 hours halloween starting \n",
      "Topic 18: \n",
      "customer worst service nobody rude doubt entire fuck unaccompanied staff \n",
      "Topic 19: \n",
      "boarded world super needed helpful direct training departing sleep could "
     ]
    }
   ],
   "source": [
    "vocab = vect.get_feature_names()\n",
    "for i, comp in enumerate(lda_model.components_):\n",
    "     vocab_comp = zip(vocab, comp)\n",
    "     sorted_words = sorted(vocab_comp, key= lambda x:x[1], reverse=True)[:10]\n",
    "     print(\"\\nTopic \"+str(i)+\": \")\n",
    "     for t in sorted_words:\n",
    "            print(t[0],end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.12 ('cloudEnv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "01244e9f708f4d2eb5343762b007acc273faa96b0b0f955667ad4252aff017a8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
