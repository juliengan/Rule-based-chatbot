{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xcjYQC301n8E",
        "outputId": "a5e058cc-dc65-4287-8168-cbcc276f85a2"
      },
      "outputs": [],
      "source": [
        "import bs4 as bs\n",
        "import urllib.request\n",
        "import pandas as pd\n",
        "#Open the cat web data page\n",
        "cat_data = urllib.request.urlopen('https://simple.wikipedia.org/wiki/Cat').read()\n",
        "\n",
        "# Retrieve the questions and answers\n",
        "tweet_data = pd.read_csv(\"question_responce.csv\")\n",
        "tweet_data = tweet_data[[\"question\",\"responce\"]]\n",
        "#Find all the paragraph html from the web page\n",
        "cat_data_paragraphs  = bs.BeautifulSoup(cat_data,\"html.parser\").find_all('p')\n",
        "#Creating the corpus of all the web page paragraphs\n",
        "cat_text = ''\n",
        "#Creating lower text corpus of cat paragraphs\n",
        "for p in cat_data_paragraphs:\n",
        "    cat_text += p.text.lower()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "Z8RyD-zY5MB9",
        "outputId": "09740f14-f45f-4c6b-d36e-7dd902ee6b0c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>question</th>\n",
              "      <th>responce</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@AmericanAir Erica on the lax team is amazing ...</td>\n",
              "      <td>@115904 We'll be sure to pass along your kind ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>@AmericanAir Could you have someone on your la...</td>\n",
              "      <td>@115904 Our apologies for the delay in respond...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Ben Tennyson and an American Airlines pilot. ðŸŽƒ...</td>\n",
              "      <td>@115905 Aww, that's definitely a future pilot ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>@AmericanAir Right, but I earned those. I also...</td>\n",
              "      <td>@115906 We're sorry for your frustration.</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Thank you, @AmericanAir for playing #ThisIsUs ...</td>\n",
              "      <td>@115909 We're glad you got to kick back and en...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            question  \\\n",
              "0  @AmericanAir Erica on the lax team is amazing ...   \n",
              "1  @AmericanAir Could you have someone on your la...   \n",
              "2  Ben Tennyson and an American Airlines pilot. ðŸŽƒ...   \n",
              "3  @AmericanAir Right, but I earned those. I also...   \n",
              "4  Thank you, @AmericanAir for playing #ThisIsUs ...   \n",
              "\n",
              "                                            responce  \n",
              "0  @115904 We'll be sure to pass along your kind ...  \n",
              "1  @115904 Our apologies for the delay in respond...  \n",
              "2  @115905 Aww, that's definitely a future pilot ...  \n",
              "3          @115906 We're sorry for your frustration.  \n",
              "4  @115909 We're glad you got to kick back and en...  "
            ]
          },
          "execution_count": 72,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "tweet_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "pandas.core.frame.DataFrame"
            ]
          },
          "execution_count": 73,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "type(tweet_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "bQQc0-qK1pgq"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\lubot\\AppData\\Local\\Temp\\ipykernel_3604\\2454954073.py:3: FutureWarning: The default value of regex will change from True to False in a future version.\n",
            "  tweet_data['responce'] = tweet_data['responce'].map(lambda x: re.sub(r'\\W+', ' ', x)).str.replace('\\d+', '')\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "tweet_data['question'] = tweet_data['question'].map(lambda x: re.sub(r'\\W+', ' ', x))\n",
        "tweet_data['responce'] = tweet_data['responce'].map(lambda x: re.sub(r'\\W+', ' ', x)).str.replace('\\d+', '')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9fFyW-JI1s4M",
        "outputId": "6a05bd3e-e871-4442-c556-7ebe51416290"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\lubot\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "response_sentences = nltk.sent_tokenize(str(tweet_data.responce))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['0         We ll be sure to pass along your kind words ...\\n1         Our apologies for the delay in responding to...\\n2         Aww that s definitely a future pilot in the ...\\n3                       We re sorry for your frustration \\n4         We re glad you got to kick back and enjoy a ...\\n                              ...                        \\n1847      We know staying connected is important why n...\\n1848      We ve capped our fares for nonstop flights a...\\n1849      Please give our Baggage team a call at    fo...\\n1850      Our apologies for the hold Our Central Bagga...\\n1851      We re providing waivers for St Croix Gillian...\\nName: responce, Length: 1852, dtype: object']\n"
          ]
        }
      ],
      "source": [
        "print(response_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "id": "DO7yD3w41wTc"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "def chatbot_answer(user_query):\n",
        "    \n",
        "    #Append the query to the sentences list\n",
        "    response_sentences.append(user_query)\n",
        "    #Create the sentences vector based on the list\n",
        "    vectorizer = TfidfVectorizer()\n",
        "    sentences_vectors = vectorizer.fit_transform(response_sentences)\n",
        "    \n",
        "    #Measure the cosine similarity and take the second closest index because the first index is the user query\n",
        "    vector_values = cosine_similarity(sentences_vectors[-1], sentences_vectors)\n",
        "    answer = response_sentences[vector_values.argsort()[0][-2]]\n",
        "    #Final check to make sure there are result present. If all the result are 0, means the text input by us are not captured in the corpus\n",
        "    input_check = vector_values.flatten()\n",
        "    input_check.sort()\n",
        "    \n",
        "    if input_check[-2] == 0:\n",
        "        return \"Please Try again\"\n",
        "    else: \n",
        "        return answer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 554
        },
        "id": "SeqvMuFB16rk",
        "outputId": "ea3cf894-e192-48f9-bbf8-862634bd51c8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello, I am the Airplane Chatbot. How can I help you ?\n",
            "Chatbot: See You Again\n"
          ]
        }
      ],
      "source": [
        "print(\"Hello, I am the Airplane Chatbot. How can I help you ?\")\n",
        "while(True):\n",
        "    query = input().lower()\n",
        "    if query not in ['bye', 'good bye', 'take care']:\n",
        "        print(\"Chatbot: \", end=\"\")\n",
        "        oui = chatbot_answer(query)\n",
        "        response_sentences.remove(query)\n",
        "    else:\n",
        "        print(\"See You Again\")\n",
        "        break\n",
        "\n",
        "with open(\"Output.txt\", \"w\") as text_file:\n",
        "    text_file.write(oui)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "id": "k64bawC92K7M"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['0         We ll be sure to pass along your kind words ...\\n1         Our apologies for the delay in responding to...\\n2         Aww that s definitely a future pilot in the ...\\n3                       We re sorry for your frustration \\n4         We re glad you got to kick back and enjoy a ...\\n                              ...                        \\n1847      We know staying connected is important why n...\\n1848      We ve capped our fares for nonstop flights a...\\n1849      Please give our Baggage team a call at    fo...\\n1850      Our apologies for the hold Our Central Bagga...\\n1851      We re providing waivers for St Croix Gillian...\\nName: responce, Length: 1852, dtype: object']\n"
          ]
        }
      ],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "chatbot.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.12 ('cloudEnv')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "vscode": {
      "interpreter": {
        "hash": "01244e9f708f4d2eb5343762b007acc273faa96b0b0f955667ad4252aff017a8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
