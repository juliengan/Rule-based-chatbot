{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\khauv\\anaconda3\\lib\\site-packages (1.7.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\khauv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\khauv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\khauv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\khauv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%pip install emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "nltk.download('punkt')\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import emoji\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "stop_words=set(stopwords.words('english'))\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Doc2Vec model\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       Unnamed: 0                                           responce  \\\n",
       "0            603  @115904 We'll be sure to pass along your kind ...   \n",
       "1            605  @115904 Our apologies for the delay in respond...   \n",
       "2            608  @115905 Aww, that's definitely a future pilot ...   \n",
       "3            612          @115906 We're sorry for your frustration.   \n",
       "4            618  @115909 We're glad you got to kick back and en...   \n",
       "...          ...                                                ...   \n",
       "1847      201947  @172376 We know staying connected is important...   \n",
       "1848      203418  @172677 We've capped our fares for nonstop fli...   \n",
       "1849      203504  @143005 Please give our Baggage team a call at...   \n",
       "1850      203506  @143005 Our apologies for the hold. Our Centra...   \n",
       "1851      203633  @172730 We're providing waivers for St Croix, ...   \n",
       "\n",
       "                                               question  \n",
       "0     @AmericanAir Erica on the lax team is amazing ...  \n",
       "1     @AmericanAir Could you have someone on your la...  \n",
       "2     Ben Tennyson and an American Airlines pilot. üéÉ...  \n",
       "3     @AmericanAir Right, but I earned those. I also...  \n",
       "4     Thank you, @AmericanAir for playing #ThisIsUs ...  \n",
       "...                                                 ...  \n",
       "1847  @AmericanAir and @172 have nailed in the trans...  \n",
       "1848  @AmericanAir Average price of ticket out: $250...  \n",
       "1849  @AmericanAir Really annoyed been over a month ...  \n",
       "1850  @AmericanAir terrible service wait ages trying...  \n",
       "1851  @AmericanAir charges their patrons to change t...  \n",
       "\n",
       "[1852 rows x 3 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('question_responce.csv')\n",
    "df.head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                     [erica, team, amazing, give, raise]\n",
      "1       [could, someone, team, available, guide, gate,...\n",
      "2       [tennyson, american, airline, pilot, trunkortr...\n",
      "3       [right, earned, also, pas, spouse, need, chang...\n",
      "4       [thank, playing, thisisus, great, flight, atte...\n",
      "                              ...                        \n",
      "1847    [nailed, transatlantic, wifi, service, able, j...\n",
      "1848                             [average, price, ticket]\n",
      "1849    [really, annoyed, month, since, damaged, claim...\n",
      "1850    [terrible, service, wait, age, trying, call, n...\n",
      "1851    [charge, patron, change, flight, every, time, ...\n",
      "Name: question, Length: 1852, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def give_emoji_free_text(text):\n",
    "    return emoji.get_emoji_regexp().sub(r'', text)\n",
    "    \n",
    "def preprocess(string):\n",
    "    emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth      \n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "    string = re.sub(r'\\s+', ' ', string)\n",
    "    string = re.sub(r'\\[[0-9]*\\]', '', string)\n",
    "    string = re.sub(r'(@([A-Za-z0-9`~!@#$%^&*()_|+\\-=?;:\\'\",.<>\\{\\}\\[\\]\\\\\\/]{2,32}))', '', string) # remove tags\n",
    "    string = re.sub(r'\\d+', '', string) # remove numbers\n",
    "    string = re.sub(r'\\#+', '', string) # remove hashtags\n",
    "    string = re.sub(r'http\\S+', '', string) # remove urls\n",
    "    string = re.sub(emoticon_string,'', string)# remove emojis\n",
    "    string = simple_preprocess(string,deacc=True) #tokenizing, lowercasing, removing accents\n",
    "\n",
    "    lemma_function = WordNetLemmatizer()\n",
    "    lemma = [lemma_function.lemmatize(w) for w in string if w not in stop_words and len(w) > 3]\n",
    "    return lemma\n",
    "\n",
    "text_data = [] \n",
    "text_data = df['question'].apply(preprocess) #preprocessing\n",
    "\n",
    "print(text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intents / Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel# spaCy for preprocessing\n",
    "import spacy# Plotting tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(text_data)\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "\n",
    "\n",
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, '0.033*\"flight\" + 0.020*\"wifi\" + 0.013*\"look\" + 0.013*\"good\"')\n",
      "(53, '0.025*\"password\" + 0.019*\"special\" + 0.019*\"character\" + 0.019*\"flight\"')\n",
      "(57, '0.068*\"flight\" + 0.022*\"cool\" + 0.016*\"pilot\" + 0.016*\"first\"')\n",
      "(71, '0.045*\"flight\" + 0.020*\"need\" + 0.015*\"make\" + 0.010*\"switch\"')\n",
      "(42, '0.053*\"flight\" + 0.018*\"stop\" + 0.014*\"part\" + 0.014*\"american\"')\n",
      "(18, '0.036*\"flight\" + 0.017*\"home\" + 0.017*\"customer\" + 0.016*\"thanks\"')\n",
      "(47, '0.026*\"lost\" + 0.026*\"flight\" + 0.026*\"plane\" + 0.017*\"baggage\"')\n",
      "(31, '0.017*\"luggage\" + 0.017*\"flight\" + 0.011*\"upgraded\" + 0.011*\"customer\"')\n",
      "(4, '0.048*\"flight\" + 0.015*\"thanks\" + 0.015*\"gate\" + 0.013*\"great\"')\n",
      "(84, '0.032*\"flight\" + 0.015*\"change\" + 0.015*\"never\" + 0.010*\"hour\"')\n",
      "(54, '0.026*\"flight\" + 0.016*\"without\" + 0.016*\"need\" + 0.013*\"delay\"')\n",
      "(22, '0.058*\"flight\" + 0.029*\"seat\" + 0.019*\"like\" + 0.018*\"time\"')\n",
      "(79, '0.031*\"flight\" + 0.022*\"time\" + 0.022*\"gate\" + 0.013*\"service\"')\n",
      "(68, '0.031*\"flight\" + 0.029*\"gate\" + 0.017*\"thank\" + 0.017*\"change\"')\n",
      "(37, '0.033*\"flight\" + 0.020*\"check\" + 0.020*\"charged\" + 0.015*\"getting\"')\n",
      "(40, '0.030*\"flight\" + 0.023*\"service\" + 0.021*\"customer\" + 0.017*\"social\"')\n",
      "(28, '0.027*\"flight\" + 0.016*\"help\" + 0.011*\"book\" + 0.011*\"service\"')\n",
      "(17, '0.024*\"city\" + 0.024*\"second\" + 0.019*\"flight\" + 0.016*\"putting\"')\n",
      "(59, '0.034*\"flight\" + 0.016*\"help\" + 0.013*\"home\" + 0.011*\"ticket\"')\n",
      "(39, '0.039*\"flight\" + 0.018*\"would\" + 0.014*\"voucher\" + 0.014*\"meal\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 100\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary)\n",
    "ldamodel.save('model5.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La niveau de coh√©rence est assez pas. Passons au LdaMallet qui doit donner de meilleurs r√©sultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LdaMallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, [('delay', 0.11093247588424437), ('hour', 0.0819935691318328), ('issue', 0.07877813504823152), ('problem', 0.04823151125401929), ('told', 0.03536977491961415), ('missed', 0.03054662379421222), ('family', 0.028938906752411574), ('maintenance', 0.02572347266881029), ('ground', 0.01929260450160772), ('international', 0.017684887459807074)]), (16, [('upgrade', 0.08452950558213716), ('love', 0.049441786283891544), ('work', 0.0430622009569378), ('food', 0.03987240829346093), ('platinum', 0.028708133971291867), ('experience', 0.023923444976076555), ('amazing', 0.023923444976076555), ('terminal', 0.02073365231259968), ('awesome', 0.02073365231259968), ('landing', 0.017543859649122806)]), (7, [('flight', 0.27436281859070466), ('delayed', 0.095952023988006), ('hour', 0.07496251874062969), ('connection', 0.034482758620689655), ('miss', 0.029985007496251874), ('money', 0.025487256371814093), ('reason', 0.022488755622188907), ('december', 0.01649175412293853), ('scheduled', 0.01649175412293853), ('weekend', 0.014992503748125937)]), (11, [('great', 0.11077389984825493), ('free', 0.05311077389984825), ('good', 0.05007587253414264), ('happy', 0.04552352048558422), ('staff', 0.03945371775417299), ('thing', 0.03793626707132018), ('team', 0.03338391502276176), ('hope', 0.03338391502276176), ('guy', 0.02276176024279211), ('taking', 0.019726858877086494)]), (4, [('flight', 0.2352), ('give', 0.0512), ('nice', 0.0512), ('long', 0.0304), ('extra', 0.0304), ('cool', 0.0208), ('awful', 0.0192), ('giving', 0.0192), ('connecting', 0.0192), ('super', 0.0192)]), (15, [('time', 0.2033898305084746), ('minute', 0.0662557781201849), ('year', 0.061633281972265024), ('waiting', 0.05701078582434515), ('made', 0.04314329738058552), ('wifi', 0.02465331278890601), ('response', 0.023112480739599383), ('company', 0.01848998459167951), ('information', 0.01694915254237288), ('plan', 0.015408320493066256)]), (19, [('airline', 0.11738484398216939), ('class', 0.07578008915304606), ('american', 0.0475482912332838), ('business', 0.03714710252600297), ('lounge', 0.03268945022288262), ('week', 0.029717682020802376), ('update', 0.022288261515601784), ('small', 0.020802377414561663), ('medium', 0.017830609212481426), ('flagship', 0.01634472511144131)]), (13, [('flight', 0.1847457627118644), ('baggage', 0.04915254237288136), ('miami', 0.04745762711864407), ('luggage', 0.04745762711864407), ('morning', 0.02711864406779661), ('checked', 0.025423728813559324), ('hold', 0.025423728813559324), ('stuck', 0.020338983050847456), ('booked', 0.020338983050847456), ('website', 0.01864406779661017)]), (10, [('travel', 0.08333333333333333), ('pilot', 0.0696969696969697), ('refund', 0.051515151515151514), ('holiday', 0.05), ('book', 0.04090909090909091), ('full', 0.03333333333333333), ('cancel', 0.03333333333333333), ('chance', 0.024242424242424242), ('cancellation', 0.021212121212121213), ('affected', 0.021212121212121213)]), (6, [('plane', 0.1839080459770115), ('late', 0.042692939244663386), ('hour', 0.0361247947454844), ('left', 0.026272577996715927), ('broken', 0.024630541871921183), ('suck', 0.022988505747126436), ('friend', 0.014778325123152709), ('twitter', 0.014778325123152709), ('canceled', 0.014778325123152709), ('provide', 0.014778325123152709)])]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "os.environ['MALLET_HOME'] = 'C:/Users/khauv/Documents/Github/Rule-based-chatbot/mallet-2.0.8/'\n",
    "mallet_path = 'C:/Users/khauv/Documents/Github/Rule-based-chatbot/mallet-2.0.8/bin/mallet' # update this path\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=dictionary)\n",
    "\n",
    "# Show Topics\n",
    "print(ldamallet.show_topics(formatted=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fonction pour evaluer le niveau de coh√©rence du model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.5792077373285466\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=text_data, dictionary=dictionary, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fonction pour √©valuer le niveau de coh√©rence du model dans un interval donn√©e de nombres de topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Graphique entre la coh√©rence et le nombre de topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=text_data, start=120, limit=320, step=40)\n",
    "import matplotlib.pyplot as plt\n",
    "# Show graph\n",
    "limit=320; start=120; step=40;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On choisit le nombre de topics l√† o√π la courbe commence √† stabiliser, ici 240."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Les 240 topics du model que l'on a choisit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(181, '0.323*\"card\" + 0.169*\"stop\" + 0.154*\"credit\" + 0.015*\"website\" + 0.015*\"elite\" + 0.015*\"town\" + 0.015*\"back\" + 0.015*\"flight\" + 0.015*\"worried\" + 0.015*\"updated\"'), (38, '0.396*\"time\" + 0.042*\"exceptional\" + 0.042*\"apparently\" + 0.042*\"state\" + 0.042*\"reach\" + 0.021*\"takeoff\" + 0.021*\"exit\" + 0.021*\"salida\" + 0.021*\"people\" + 0.021*\"return\"'), (14, '0.396*\"hour\" + 0.094*\"water\" + 0.075*\"carrier\" + 0.057*\"checked\" + 0.038*\"yesterday\" + 0.019*\"penalty\" + 0.019*\"prouder\" + 0.019*\"drop\" + 0.019*\"minute\" + 0.019*\"middle\"'), (198, '0.171*\"airport\" + 0.129*\"seat\" + 0.071*\"board\" + 0.071*\"plane\" + 0.057*\"left\" + 0.043*\"malfunction\" + 0.029*\"mechanical\" + 0.029*\"leaf\" + 0.029*\"waiting\" + 0.014*\"assignment\"'), (173, '0.213*\"time\" + 0.128*\"missed\" + 0.085*\"miami\" + 0.085*\"receive\" + 0.064*\"place\" + 0.043*\"disappointed\" + 0.043*\"girlfriend\" + 0.043*\"waiting\" + 0.021*\"rockstars\" + 0.021*\"description\"'), (159, '0.421*\"check\" + 0.211*\"bag\" + 0.053*\"forced\" + 0.053*\"full\" + 0.032*\"destination\" + 0.032*\"sitting\" + 0.021*\"guy\" + 0.011*\"icym\" + 0.011*\"lining\" + 0.011*\"important\"'), (80, '0.244*\"point\" + 0.146*\"paid\" + 0.098*\"trip\" + 0.073*\"people\" + 0.049*\"boarded\" + 0.024*\"concerned\" + 0.024*\"fair\" + 0.024*\"confuse\" + 0.024*\"bday\" + 0.024*\"home\"'), (121, '0.214*\"flight\" + 0.119*\"worried\" + 0.071*\"late\" + 0.071*\"staff\" + 0.071*\"miami\" + 0.048*\"inside\" + 0.024*\"original\" + 0.024*\"agent\" + 0.024*\"meyers\" + 0.024*\"effort\"'), (87, '0.306*\"work\" + 0.210*\"love\" + 0.065*\"trip\" + 0.048*\"headphone\" + 0.032*\"today\" + 0.032*\"upset\" + 0.032*\"avoid\" + 0.032*\"half\" + 0.016*\"extending\" + 0.016*\"nosop\"'), (81, '0.255*\"plane\" + 0.106*\"fault\" + 0.085*\"chicago\" + 0.043*\"regularly\" + 0.043*\"calling\" + 0.043*\"absolute\" + 0.043*\"mistake\" + 0.021*\"exit\" + 0.021*\"increased\" + 0.021*\"shared\"'), (125, '0.119*\"boarding\" + 0.095*\"meal\" + 0.071*\"reservation\" + 0.048*\"ride\" + 0.024*\"nowthx\" + 0.024*\"longer\" + 0.024*\"esta\" + 0.024*\"glad\" + 0.024*\"rock\" + 0.024*\"service\"'), (42, '0.510*\"free\" + 0.039*\"taking\" + 0.020*\"experience\" + 0.020*\"foot\" + 0.020*\"payment\" + 0.020*\"citi\" + 0.020*\"explicacion\" + 0.020*\"excuse\" + 0.020*\"coming\" + 0.020*\"matter\"'), (232, '0.114*\"make\" + 0.086*\"mile\" + 0.057*\"flight\" + 0.057*\"comfortable\" + 0.057*\"made\" + 0.057*\"asap\" + 0.057*\"based\" + 0.029*\"livery\" + 0.029*\"esperaba\" + 0.029*\"door\"'), (92, '0.176*\"special\" + 0.118*\"terrible\" + 0.078*\"character\" + 0.078*\"policy\" + 0.059*\"wifi\" + 0.039*\"save\" + 0.039*\"identify\" + 0.020*\"jet\" + 0.020*\"tweeted\" + 0.020*\"unnecessarily\"'), (55, '0.529*\"seat\" + 0.100*\"back\" + 0.029*\"hacer\" + 0.029*\"bought\" + 0.014*\"career\" + 0.014*\"chat\" + 0.014*\"part\" + 0.014*\"supervision\" + 0.014*\"added\" + 0.014*\"surprised\"'), (216, '0.085*\"send\" + 0.068*\"unacceptable\" + 0.068*\"find\" + 0.068*\"issuing\" + 0.051*\"forward\" + 0.051*\"thanksgiving\" + 0.051*\"mechanical\" + 0.034*\"inform\" + 0.034*\"flying\" + 0.034*\"caste\"'), (83, '0.259*\"maintenance\" + 0.103*\"tarmac\" + 0.052*\"status\" + 0.034*\"start\" + 0.034*\"outlet\" + 0.034*\"speaking\" + 0.034*\"worse\" + 0.034*\"dropped\" + 0.034*\"france\" + 0.017*\"exceeding\"'), (197, '0.128*\"phone\" + 0.106*\"flight\" + 0.085*\"amazing\" + 0.064*\"minor\" + 0.064*\"comment\" + 0.043*\"good\" + 0.021*\"adding\" + 0.021*\"insult\" + 0.021*\"switch\" + 0.021*\"oppose\"'), (115, '0.294*\"club\" + 0.162*\"admiral\" + 0.059*\"team\" + 0.044*\"claim\" + 0.029*\"guy\" + 0.029*\"tonight\" + 0.029*\"class\" + 0.015*\"causing\" + 0.015*\"cheese\" + 0.015*\"early\"'), (239, '0.122*\"refund\" + 0.122*\"blah\" + 0.073*\"half\" + 0.049*\"company\" + 0.049*\"entire\" + 0.049*\"terminal\" + 0.049*\"reduce\" + 0.049*\"guess\" + 0.049*\"finally\" + 0.024*\"proper\"')]\n"
     ]
    }
   ],
   "source": [
    "# Select the model and print the topics\n",
    "optimal_model = model_list[3]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "print(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Attribution des topics pour tous les messages du dataset dans un m√™me dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>give, gate, back, announced, miss, wing, fille...</td>\n",
       "      <td>@AmericanAir Erica on the lax team is amazing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>make, mile, flight, comfortable, made, asap, b...</td>\n",
       "      <td>@AmericanAir Could you have someone on your la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>235.0</td>\n",
       "      <td>0.0381</td>\n",
       "      <td>airline, american, week, contact, reconocerlo,...</td>\n",
       "      <td>Ben Tennyson and an American Airlines pilot. üéÉ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>back, flyer, frequent, program, trip, mile, bu...</td>\n",
       "      <td>@AmericanAir Right, but I earned those. I also...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0227</td>\n",
       "      <td>back, attendant, voucher, wanted, checked, mai...</td>\n",
       "      <td>Thank you, @AmericanAir for playing #ThisIsUs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>wifi, making, delta, dont, loyal, easy, teach,...</td>\n",
       "      <td>@AmericanAir's wifi makes Amtrak's wifi look p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>115.0</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>club, admiral, team, claim, guy, tonight, clas...</td>\n",
       "      <td>Wonderful club! @americanair (@ American Airli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0251</td>\n",
       "      <td>attendant, visit, finally, shit, changed, peop...</td>\n",
       "      <td>@AmericanAir already did...changed browsers, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>hold, return, family, enjoy, browser, reservat...</td>\n",
       "      <td>@AmericanAir ........still....on....hold.....t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>ticket, delayed, worst, stop, expectation, buy...</td>\n",
       "      <td>@AmericanAir well now i am told the ticket cos...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0           226.0              0.0159   \n",
       "1            1           232.0              0.0121   \n",
       "2            2           235.0              0.0381   \n",
       "3            3             1.0              0.0139   \n",
       "4            4           192.0              0.0227   \n",
       "5            5            74.0              0.0600   \n",
       "6            6           115.0              0.0582   \n",
       "7            7            65.0              0.0251   \n",
       "8            8            69.0              0.0121   \n",
       "9            9            34.0              0.0156   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  give, gate, back, announced, miss, wing, fille...   \n",
       "1  make, mile, flight, comfortable, made, asap, b...   \n",
       "2  airline, american, week, contact, reconocerlo,...   \n",
       "3  back, flyer, frequent, program, trip, mile, bu...   \n",
       "4  back, attendant, voucher, wanted, checked, mai...   \n",
       "5  wifi, making, delta, dont, loyal, easy, teach,...   \n",
       "6  club, admiral, team, claim, guy, tonight, clas...   \n",
       "7  attendant, visit, finally, shit, changed, peop...   \n",
       "8  hold, return, family, enjoy, browser, reservat...   \n",
       "9  ticket, delayed, worst, stop, expectation, buy...   \n",
       "\n",
       "                                                Text  \n",
       "0  @AmericanAir Erica on the lax team is amazing ...  \n",
       "1  @AmericanAir Could you have someone on your la...  \n",
       "2  Ben Tennyson and an American Airlines pilot. üéÉ...  \n",
       "3  @AmericanAir Right, but I earned those. I also...  \n",
       "4  Thank you, @AmericanAir for playing #ThisIsUs ...  \n",
       "5  @AmericanAir's wifi makes Amtrak's wifi look p...  \n",
       "6  Wonderful club! @americanair (@ American Airli...  \n",
       "7  @AmericanAir already did...changed browsers, d...  \n",
       "8  @AmericanAir ........still....on....hold.....t...  \n",
       "9  @AmericanAir well now i am told the ticket cos...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel=ldamallet, corpus=corpus, texts=text_data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=df.question.values.tolist())\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ajout de la colonne 'responce' dans le dataframe r√©alis√© precedemment contenant les questions et leur topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "      <th>responce</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>give, gate, back, announced, miss, wing, fille...</td>\n",
       "      <td>@AmericanAir Erica on the lax team is amazing ...</td>\n",
       "      <td>@115904 We'll be sure to pass along your kind ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>make, mile, flight, comfortable, made, asap, b...</td>\n",
       "      <td>@AmericanAir Could you have someone on your la...</td>\n",
       "      <td>@115904 Our apologies for the delay in respond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>235.0</td>\n",
       "      <td>0.0381</td>\n",
       "      <td>airline, american, week, contact, reconocerlo,...</td>\n",
       "      <td>Ben Tennyson and an American Airlines pilot. üéÉ...</td>\n",
       "      <td>@115905 Aww, that's definitely a future pilot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>back, flyer, frequent, program, trip, mile, bu...</td>\n",
       "      <td>@AmericanAir Right, but I earned those. I also...</td>\n",
       "      <td>@115906 We're sorry for your frustration.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0227</td>\n",
       "      <td>back, attendant, voucher, wanted, checked, mai...</td>\n",
       "      <td>Thank you, @AmericanAir for playing #ThisIsUs ...</td>\n",
       "      <td>@115909 We're glad you got to kick back and en...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0           226.0              0.0159   \n",
       "1            1           232.0              0.0121   \n",
       "2            2           235.0              0.0381   \n",
       "3            3             1.0              0.0139   \n",
       "4            4           192.0              0.0227   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  give, gate, back, announced, miss, wing, fille...   \n",
       "1  make, mile, flight, comfortable, made, asap, b...   \n",
       "2  airline, american, week, contact, reconocerlo,...   \n",
       "3  back, flyer, frequent, program, trip, mile, bu...   \n",
       "4  back, attendant, voucher, wanted, checked, mai...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  @AmericanAir Erica on the lax team is amazing ...   \n",
       "1  @AmericanAir Could you have someone on your la...   \n",
       "2  Ben Tennyson and an American Airlines pilot. üéÉ...   \n",
       "3  @AmericanAir Right, but I earned those. I also...   \n",
       "4  Thank you, @AmericanAir for playing #ThisIsUs ...   \n",
       "\n",
       "                                            responce  \n",
       "0  @115904 We'll be sure to pass along your kind ...  \n",
       "1  @115904 Our apologies for the delay in respond...  \n",
       "2  @115905 Aww, that's definitely a future pilot ...  \n",
       "3          @115906 We're sorry for your frustration.  \n",
       "4  @115909 We're glad you got to kick back and en...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_df= df_dominant_topic.join(df['responce'], lsuffix='_database', rsuffix='_input')\n",
    "complete_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input de l'utilisateur et son topic (A faire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>flight, captain, find, send, price, passed, wi...</td>\n",
       "      <td>@AmericanAir Erica on the lax team is amazing ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0           142.0              0.0147   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  flight, captain, find, send, price, passed, wi...   \n",
       "\n",
       "                                                Text  \n",
       "0  @AmericanAir Erica on the lax team is amazing ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userinput=preprocess(\"I have passed an agreable moment with you! Thank you!\")\n",
    "dictionaryInput = corpora.Dictionary([userinput])\n",
    "corpusInput = [dictionary.doc2bow(userinput)]\n",
    "\n",
    "df_topic_test = format_topics_sentences(ldamodel=optimal_model, corpus=corpusInput, texts=\"@AmericanAir Erica on the lax team is amazing give her a raise ty\")\n",
    "\n",
    "# Format\n",
    "df_dominant_topic_test = df_topic_test.reset_index()\n",
    "df_dominant_topic_test.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic_test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Filtrage du dataframe selon un topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "      <th>responce</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>546</td>\n",
       "      <td>176.0</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>miami, flying, changing, issue, seat, reward, ...</td>\n",
       "      <td>Does anyone know how we can check to see if ou...</td>\n",
       "      <td>@136438 We're working on this issue and expect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1843</th>\n",
       "      <td>1843</td>\n",
       "      <td>176.0</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>miami, flying, changing, issue, seat, reward, ...</td>\n",
       "      <td>Without a doubt @AmericanAir has the worst rew...</td>\n",
       "      <td>@172158 We strive for a great rewards program,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "546           546           176.0              0.0139   \n",
       "1843         1843           176.0              0.0162   \n",
       "\n",
       "                                               Keywords  \\\n",
       "546   miami, flying, changing, issue, seat, reward, ...   \n",
       "1843  miami, flying, changing, issue, seat, reward, ...   \n",
       "\n",
       "                                                   Text  \\\n",
       "546   Does anyone know how we can check to see if ou...   \n",
       "1843  Without a doubt @AmericanAir has the worst rew...   \n",
       "\n",
       "                                               responce  \n",
       "546   @136438 We're working on this issue and expect...  \n",
       "1843  @172158 We strive for a great rewards program,...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oui = complete_df[complete_df['Dominant_Topic']==176.0]\n",
    "oui.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):    \n",
    "    emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth      \n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "    \n",
    "    # Remove stopwords \n",
    "    answers  = []\n",
    "    for i in tweet_data.responce:\n",
    "        i = give_emoji_free_text(i).lower()\n",
    "        answers.append(re.sub(r'\\s+', ' ',\n",
    "                            re.sub(r'\\[[0-9]*\\]', ' ',\n",
    "                            re.sub(r'(@([A-Za-z0-9`~!@#$%^&*()_|+\\-=?;:\\'\",.<>\\{\\}\\[\\]\\\\\\/]{2,32}))', '', # remove tags\n",
    "                            re.sub(r'\\d+', ' ', # remove numbers\n",
    "                            re.sub(r'\\#+', ' ', # remove hashtags\n",
    "                            re.sub(r'http\\S+', ' ', # remove urls\n",
    "                            re.sub(emoticon_string,'', # remove emojis\n",
    "                            re.sub(r'[\\.\\?\\!\\,\\:\\;\\\"]', '', i) # remove punctuation\n",
    "                                  ))))))))\n",
    "    # Lemmatization and tokenization\n",
    "    sentences = []\n",
    "    le = WordNetLemmatizer()\n",
    "    for i in answers:\n",
    "        word_tokens = word_tokenize(i)\n",
    "        lemmas = [ le.lemmatize(w) for w in word_tokens if w not in stop_words and len(w) > 3 ]\n",
    "        cleaned_text=\" \".join(lemmas)\n",
    "        sentences.append(cleaned_text)\n",
    "    \n",
    "\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['working issue expect avoid cancellation', 'strive great reward program hear liking offer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-eb7aabf9e3c4>:2: DeprecationWarning: 'emoji.get_emoji_regexp()' is deprecated and will be removed in version 2.0.0. If you want to remove emoji from a string, consider the method emoji.replace_emoji(str, replace='').\n",
      "To hide this warning, pin/downgrade the package to 'emoji~=1.6.3'\n",
      "  return emoji.get_emoji_regexp().sub(r'', text)\n"
     ]
    }
   ],
   "source": [
    "tweet_data = oui[[\"Text\",\"responce\"]]\n",
    "responce_data = clean_text(tweet_data.responce) #preprocessing\n",
    "\n",
    "print(responce_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec model\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_corpus = preprocess_documents(responce_data)\n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(responce_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words='working issue expect avoid cancellation', tags=[0]),\n",
       " TaggedDocument(words='strive great reward program hear liking offer', tags=[1])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cannot sort vocabulary after model weights already initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-134327dcb371>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      7\u001b[0m                 dm =0) # PV-Bag of words used\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtagged_data\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[0mtrain_corpus\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mtaggeddoc\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtaggeddoc_list\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtagged_data\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mtaggeddoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtaggeddoc_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\khauv\\anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mbuild_vocab\u001b[1;34m(self, documents, corpus_file, update, progress_per, keep_raw_vocab, trim_rule, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpus_count\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1187\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcorpus_total_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1188\u001b[1;33m         report_values = self.vocabulary.prepare_vocab(\n\u001b[0m\u001b[0;32m   1189\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnegative\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkeep_raw_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mkeep_raw_vocab\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrim_rule\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrim_rule\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1190\u001b[0m             **kwargs)\n",
      "\u001b[1;32mc:\\Users\\khauv\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36mprepare_vocab\u001b[1;34m(self, hs, negative, wv, update, keep_raw_vocab, trim_rule, min_count, sample, dry_run)\u001b[0m\n\u001b[0;32m   1755\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1756\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msorted_vocab\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mupdate\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1757\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1758\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1759\u001b[0m             \u001b[1;31m# add info about each word's Huffman encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\khauv\\anaconda3\\lib\\site-packages\\gensim\\models\\word2vec.py\u001b[0m in \u001b[0;36msort_vocab\u001b[1;34m(self, wv)\u001b[0m\n\u001b[0;32m   1602\u001b[0m         \u001b[1;34m\"\"\"Sort the vocabulary so the most frequent words have the lowest indexes.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1603\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1604\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cannot sort vocabulary after model weights already initialized.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1605\u001b[0m         \u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreverse\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1606\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex2word\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: cannot sort vocabulary after model weights already initialized."
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(tagged_data, \n",
    "                vector_size=200, \n",
    "                epochs=100, \n",
    "                hs=1, # use of hierachical softmax\n",
    "                min_count=1, \n",
    "                window=2, # Max distance\n",
    "                dm =0) # PV-Bag of words used\n",
    "  \n",
    "model.build_vocab(tagged_data)\n",
    "train_corpus = [taggeddoc for taggeddoc_list in tagged_data for taggeddoc in taggeddoc_list]\n",
    "\n",
    "for epoch in range(model.epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=2,\n",
    "                epochs=100)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a67b01ff655fedeafbec0f5c154c73501f94f5bc6352173beba5e66e9a24b9e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
