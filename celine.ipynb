{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: emoji in c:\\users\\khauv\\anaconda3\\lib\\site-packages (1.7.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: There was an error checking the latest version of pip.\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\khauv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\khauv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\khauv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\khauv\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "%pip install emoji\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "import random\n",
    "nltk.download('punkt')\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords  #stopwords\n",
    "from nltk.stem import WordNetLemmatizer  \n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "import emoji\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "stop_words=set(stopwords.words('english'))\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Doc2Vec model\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "from collections import defaultdict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of       Unnamed: 0                                           responce  \\\n",
       "0            603  @115904 We'll be sure to pass along your kind ...   \n",
       "1            605  @115904 Our apologies for the delay in respond...   \n",
       "2            608  @115905 Aww, that's definitely a future pilot ...   \n",
       "3            612          @115906 We're sorry for your frustration.   \n",
       "4            618  @115909 We're glad you got to kick back and en...   \n",
       "...          ...                                                ...   \n",
       "1847      201947  @172376 We know staying connected is important...   \n",
       "1848      203418  @172677 We've capped our fares for nonstop fli...   \n",
       "1849      203504  @143005 Please give our Baggage team a call at...   \n",
       "1850      203506  @143005 Our apologies for the hold. Our Centra...   \n",
       "1851      203633  @172730 We're providing waivers for St Croix, ...   \n",
       "\n",
       "                                               question  \n",
       "0     @AmericanAir Erica on the lax team is amazing ...  \n",
       "1     @AmericanAir Could you have someone on your la...  \n",
       "2     Ben Tennyson and an American Airlines pilot. üéÉ...  \n",
       "3     @AmericanAir Right, but I earned those. I also...  \n",
       "4     Thank you, @AmericanAir for playing #ThisIsUs ...  \n",
       "...                                                 ...  \n",
       "1847  @AmericanAir and @172 have nailed in the trans...  \n",
       "1848  @AmericanAir Average price of ticket out: $250...  \n",
       "1849  @AmericanAir Really annoyed been over a month ...  \n",
       "1850  @AmericanAir terrible service wait ages trying...  \n",
       "1851  @AmericanAir charges their patrons to change t...  \n",
       "\n",
       "[1852 rows x 3 columns]>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('question_responce.csv')\n",
    "df.head\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0                     [erica, team, amazing, give, raise]\n",
      "1       [could, someone, team, available, guide, gate,...\n",
      "2       [tennyson, american, airline, pilot, trunkortr...\n",
      "3       [right, earned, also, pas, spouse, need, chang...\n",
      "4       [thank, playing, thisisus, great, flight, atte...\n",
      "                              ...                        \n",
      "1847    [nailed, transatlantic, wifi, service, able, j...\n",
      "1848                             [average, price, ticket]\n",
      "1849    [really, annoyed, month, since, damaged, claim...\n",
      "1850    [terrible, service, wait, age, trying, call, n...\n",
      "1851    [charge, patron, change, flight, every, time, ...\n",
      "Name: question, Length: 1852, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def give_emoji_free_text(text):\n",
    "    return emoji.get_emoji_regexp().sub(r'', text)\n",
    "    \n",
    "def preprocess(string):\n",
    "    emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth      \n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "    string = re.sub(r'\\s+', ' ', string)\n",
    "    string = re.sub(r'\\[[0-9]*\\]', '', string)\n",
    "    string = re.sub(r'(@([A-Za-z0-9`~!@#$%^&*()_|+\\-=?;:\\'\",.<>\\{\\}\\[\\]\\\\\\/]{2,32}))', '', string) # remove tags\n",
    "    string = re.sub(r'\\d+', '', string) # remove numbers\n",
    "    string = re.sub(r'\\#+', '', string) # remove hashtags\n",
    "    string = re.sub(r'http\\S+', '', string) # remove urls\n",
    "    string = re.sub(emoticon_string,'', string)# remove emojis\n",
    "    string = simple_preprocess(string,deacc=True) #tokenizing, lowercasing, removing accents\n",
    "\n",
    "    lemma_function = WordNetLemmatizer()\n",
    "    lemma = [lemma_function.lemmatize(w) for w in string if w not in stop_words and len(w) > 3]\n",
    "    return lemma\n",
    "\n",
    "text_data = [] \n",
    "text_data = df['question'].apply(preprocess) #preprocessing\n",
    "\n",
    "print(text_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intents / Topic Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint# Gensim\n",
    "import gensim\n",
    "import gensim.corpora as corpora\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.models import CoherenceModel# spaCy for preprocessing\n",
    "import spacy# Plotting tools\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import corpora\n",
    "dictionary = corpora.Dictionary(text_data)\n",
    "corpus = [dictionary.doc2bow(text) for text in text_data]\n",
    "\n",
    "\n",
    "import pickle\n",
    "pickle.dump(corpus, open('corpus.pkl', 'wb'))\n",
    "dictionary.save('dictionary.gensim')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LdaModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7, '0.033*\"flight\" + 0.020*\"wifi\" + 0.013*\"look\" + 0.013*\"good\"')\n",
      "(53, '0.025*\"password\" + 0.019*\"special\" + 0.019*\"character\" + 0.019*\"flight\"')\n",
      "(57, '0.068*\"flight\" + 0.022*\"cool\" + 0.016*\"pilot\" + 0.016*\"first\"')\n",
      "(71, '0.045*\"flight\" + 0.020*\"need\" + 0.015*\"make\" + 0.010*\"switch\"')\n",
      "(42, '0.053*\"flight\" + 0.018*\"stop\" + 0.014*\"part\" + 0.014*\"american\"')\n",
      "(18, '0.036*\"flight\" + 0.017*\"home\" + 0.017*\"customer\" + 0.016*\"thanks\"')\n",
      "(47, '0.026*\"lost\" + 0.026*\"flight\" + 0.026*\"plane\" + 0.017*\"baggage\"')\n",
      "(31, '0.017*\"luggage\" + 0.017*\"flight\" + 0.011*\"upgraded\" + 0.011*\"customer\"')\n",
      "(4, '0.048*\"flight\" + 0.015*\"thanks\" + 0.015*\"gate\" + 0.013*\"great\"')\n",
      "(84, '0.032*\"flight\" + 0.015*\"change\" + 0.015*\"never\" + 0.010*\"hour\"')\n",
      "(54, '0.026*\"flight\" + 0.016*\"without\" + 0.016*\"need\" + 0.013*\"delay\"')\n",
      "(22, '0.058*\"flight\" + 0.029*\"seat\" + 0.019*\"like\" + 0.018*\"time\"')\n",
      "(79, '0.031*\"flight\" + 0.022*\"time\" + 0.022*\"gate\" + 0.013*\"service\"')\n",
      "(68, '0.031*\"flight\" + 0.029*\"gate\" + 0.017*\"thank\" + 0.017*\"change\"')\n",
      "(37, '0.033*\"flight\" + 0.020*\"check\" + 0.020*\"charged\" + 0.015*\"getting\"')\n",
      "(40, '0.030*\"flight\" + 0.023*\"service\" + 0.021*\"customer\" + 0.017*\"social\"')\n",
      "(28, '0.027*\"flight\" + 0.016*\"help\" + 0.011*\"book\" + 0.011*\"service\"')\n",
      "(17, '0.024*\"city\" + 0.024*\"second\" + 0.019*\"flight\" + 0.016*\"putting\"')\n",
      "(59, '0.034*\"flight\" + 0.016*\"help\" + 0.013*\"home\" + 0.011*\"ticket\"')\n",
      "(39, '0.039*\"flight\" + 0.018*\"would\" + 0.014*\"voucher\" + 0.014*\"meal\"')\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "NUM_TOPICS = 100\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics = NUM_TOPICS, id2word=dictionary)\n",
    "ldamodel.save('model5.gensim')\n",
    "topics = ldamodel.print_topics(num_words=4)\n",
    "for topic in topics:\n",
    "    print(topic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La niveau de coh√©rence est assez pas. Passons au LdaMallet qui doit donner de meilleurs r√©sultats."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LdaMallet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(8, [('delay', 0.11093247588424437), ('hour', 0.0819935691318328), ('issue', 0.07877813504823152), ('problem', 0.04823151125401929), ('told', 0.03536977491961415), ('missed', 0.03054662379421222), ('family', 0.028938906752411574), ('maintenance', 0.02572347266881029), ('ground', 0.01929260450160772), ('international', 0.017684887459807074)]), (16, [('upgrade', 0.08452950558213716), ('love', 0.049441786283891544), ('work', 0.0430622009569378), ('food', 0.03987240829346093), ('platinum', 0.028708133971291867), ('experience', 0.023923444976076555), ('amazing', 0.023923444976076555), ('terminal', 0.02073365231259968), ('awesome', 0.02073365231259968), ('landing', 0.017543859649122806)]), (7, [('flight', 0.27436281859070466), ('delayed', 0.095952023988006), ('hour', 0.07496251874062969), ('connection', 0.034482758620689655), ('miss', 0.029985007496251874), ('money', 0.025487256371814093), ('reason', 0.022488755622188907), ('december', 0.01649175412293853), ('scheduled', 0.01649175412293853), ('weekend', 0.014992503748125937)]), (11, [('great', 0.11077389984825493), ('free', 0.05311077389984825), ('good', 0.05007587253414264), ('happy', 0.04552352048558422), ('staff', 0.03945371775417299), ('thing', 0.03793626707132018), ('team', 0.03338391502276176), ('hope', 0.03338391502276176), ('guy', 0.02276176024279211), ('taking', 0.019726858877086494)]), (4, [('flight', 0.2352), ('give', 0.0512), ('nice', 0.0512), ('long', 0.0304), ('extra', 0.0304), ('cool', 0.0208), ('awful', 0.0192), ('giving', 0.0192), ('connecting', 0.0192), ('super', 0.0192)]), (15, [('time', 0.2033898305084746), ('minute', 0.0662557781201849), ('year', 0.061633281972265024), ('waiting', 0.05701078582434515), ('made', 0.04314329738058552), ('wifi', 0.02465331278890601), ('response', 0.023112480739599383), ('company', 0.01848998459167951), ('information', 0.01694915254237288), ('plan', 0.015408320493066256)]), (19, [('airline', 0.11738484398216939), ('class', 0.07578008915304606), ('american', 0.0475482912332838), ('business', 0.03714710252600297), ('lounge', 0.03268945022288262), ('week', 0.029717682020802376), ('update', 0.022288261515601784), ('small', 0.020802377414561663), ('medium', 0.017830609212481426), ('flagship', 0.01634472511144131)]), (13, [('flight', 0.1847457627118644), ('baggage', 0.04915254237288136), ('miami', 0.04745762711864407), ('luggage', 0.04745762711864407), ('morning', 0.02711864406779661), ('checked', 0.025423728813559324), ('hold', 0.025423728813559324), ('stuck', 0.020338983050847456), ('booked', 0.020338983050847456), ('website', 0.01864406779661017)]), (10, [('travel', 0.08333333333333333), ('pilot', 0.0696969696969697), ('refund', 0.051515151515151514), ('holiday', 0.05), ('book', 0.04090909090909091), ('full', 0.03333333333333333), ('cancel', 0.03333333333333333), ('chance', 0.024242424242424242), ('cancellation', 0.021212121212121213), ('affected', 0.021212121212121213)]), (6, [('plane', 0.1839080459770115), ('late', 0.042692939244663386), ('hour', 0.0361247947454844), ('left', 0.026272577996715927), ('broken', 0.024630541871921183), ('suck', 0.022988505747126436), ('friend', 0.014778325123152709), ('twitter', 0.014778325123152709), ('canceled', 0.014778325123152709), ('provide', 0.014778325123152709)])]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from gensim.models.wrappers import LdaMallet\n",
    "\n",
    "os.environ['MALLET_HOME'] = 'C:/Users/khauv/Documents/Github/Rule-based-chatbot/mallet-2.0.8/'\n",
    "mallet_path = 'C:/Users/khauv/Documents/Github/Rule-based-chatbot/mallet-2.0.8/bin/mallet' # update this path\n",
    "ldamallet = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=20, id2word=dictionary)\n",
    "\n",
    "# Show Topics\n",
    "print(ldamallet.show_topics(formatted=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fonction pour evaluer le niveau de coh√©rence du model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Coherence Score:  0.5792077373285466\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# Compute Coherence Score\n",
    "coherence_model_ldamallet = CoherenceModel(model=ldamallet, texts=text_data, dictionary=dictionary, coherence='c_v')\n",
    "coherence_ldamallet = coherence_model_ldamallet.get_coherence()\n",
    "print('\\nCoherence Score: ', coherence_ldamallet)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Fonction pour √©valuer le niveau de coh√©rence du model dans un interval donn√©e de nombres de topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_coherence_values(dictionary, corpus, texts, limit, start=2, step=3):\n",
    "    \"\"\"\n",
    "    Compute c_v coherence for various number of topics\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    dictionary : Gensim dictionary\n",
    "    corpus : Gensim corpus\n",
    "    texts : List of input texts\n",
    "    limit : Max num of topics\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    model_list : List of LDA topic models\n",
    "    coherence_values : Coherence values corresponding to the LDA model with respective number of topics\n",
    "    \"\"\"\n",
    "    coherence_values = []\n",
    "    model_list = []\n",
    "    for num_topics in range(start, limit, step):\n",
    "        model = gensim.models.wrappers.LdaMallet(mallet_path, corpus=corpus, num_topics=num_topics, id2word=dictionary)\n",
    "        model_list.append(model)\n",
    "        coherencemodel = CoherenceModel(model=model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "        coherence_values.append(coherencemodel.get_coherence())\n",
    "\n",
    "    return model_list, coherence_values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Graphique entre la coh√©rence et le nombre de topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_list, coherence_values = compute_coherence_values(dictionary=dictionary, corpus=corpus, texts=text_data, start=120, limit=320, step=40)\n",
    "import matplotlib.pyplot as plt\n",
    "# Show graph\n",
    "limit=320; start=120; step=40;\n",
    "x = range(start, limit, step)\n",
    "plt.plot(x, coherence_values)\n",
    "plt.xlabel(\"Num Topics\")\n",
    "plt.ylabel(\"Coherence score\")\n",
    "plt.legend((\"coherence_values\"), loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On choisit le nombre de topics l√† o√π la courbe commence √† stabiliser, ici 240."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Les 240 topics du model que l'on a choisit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(181, '0.323*\"card\" + 0.169*\"stop\" + 0.154*\"credit\" + 0.015*\"website\" + 0.015*\"elite\" + 0.015*\"town\" + 0.015*\"back\" + 0.015*\"flight\" + 0.015*\"worried\" + 0.015*\"updated\"'), (38, '0.396*\"time\" + 0.042*\"exceptional\" + 0.042*\"apparently\" + 0.042*\"state\" + 0.042*\"reach\" + 0.021*\"takeoff\" + 0.021*\"exit\" + 0.021*\"salida\" + 0.021*\"people\" + 0.021*\"return\"'), (14, '0.396*\"hour\" + 0.094*\"water\" + 0.075*\"carrier\" + 0.057*\"checked\" + 0.038*\"yesterday\" + 0.019*\"penalty\" + 0.019*\"prouder\" + 0.019*\"drop\" + 0.019*\"minute\" + 0.019*\"middle\"'), (198, '0.171*\"airport\" + 0.129*\"seat\" + 0.071*\"board\" + 0.071*\"plane\" + 0.057*\"left\" + 0.043*\"malfunction\" + 0.029*\"mechanical\" + 0.029*\"leaf\" + 0.029*\"waiting\" + 0.014*\"assignment\"'), (173, '0.213*\"time\" + 0.128*\"missed\" + 0.085*\"miami\" + 0.085*\"receive\" + 0.064*\"place\" + 0.043*\"disappointed\" + 0.043*\"girlfriend\" + 0.043*\"waiting\" + 0.021*\"rockstars\" + 0.021*\"description\"'), (159, '0.421*\"check\" + 0.211*\"bag\" + 0.053*\"forced\" + 0.053*\"full\" + 0.032*\"destination\" + 0.032*\"sitting\" + 0.021*\"guy\" + 0.011*\"icym\" + 0.011*\"lining\" + 0.011*\"important\"'), (80, '0.244*\"point\" + 0.146*\"paid\" + 0.098*\"trip\" + 0.073*\"people\" + 0.049*\"boarded\" + 0.024*\"concerned\" + 0.024*\"fair\" + 0.024*\"confuse\" + 0.024*\"bday\" + 0.024*\"home\"'), (121, '0.214*\"flight\" + 0.119*\"worried\" + 0.071*\"late\" + 0.071*\"staff\" + 0.071*\"miami\" + 0.048*\"inside\" + 0.024*\"original\" + 0.024*\"agent\" + 0.024*\"meyers\" + 0.024*\"effort\"'), (87, '0.306*\"work\" + 0.210*\"love\" + 0.065*\"trip\" + 0.048*\"headphone\" + 0.032*\"today\" + 0.032*\"upset\" + 0.032*\"avoid\" + 0.032*\"half\" + 0.016*\"extending\" + 0.016*\"nosop\"'), (81, '0.255*\"plane\" + 0.106*\"fault\" + 0.085*\"chicago\" + 0.043*\"regularly\" + 0.043*\"calling\" + 0.043*\"absolute\" + 0.043*\"mistake\" + 0.021*\"exit\" + 0.021*\"increased\" + 0.021*\"shared\"'), (125, '0.119*\"boarding\" + 0.095*\"meal\" + 0.071*\"reservation\" + 0.048*\"ride\" + 0.024*\"nowthx\" + 0.024*\"longer\" + 0.024*\"esta\" + 0.024*\"glad\" + 0.024*\"rock\" + 0.024*\"service\"'), (42, '0.510*\"free\" + 0.039*\"taking\" + 0.020*\"experience\" + 0.020*\"foot\" + 0.020*\"payment\" + 0.020*\"citi\" + 0.020*\"explicacion\" + 0.020*\"excuse\" + 0.020*\"coming\" + 0.020*\"matter\"'), (232, '0.114*\"make\" + 0.086*\"mile\" + 0.057*\"flight\" + 0.057*\"comfortable\" + 0.057*\"made\" + 0.057*\"asap\" + 0.057*\"based\" + 0.029*\"livery\" + 0.029*\"esperaba\" + 0.029*\"door\"'), (92, '0.176*\"special\" + 0.118*\"terrible\" + 0.078*\"character\" + 0.078*\"policy\" + 0.059*\"wifi\" + 0.039*\"save\" + 0.039*\"identify\" + 0.020*\"jet\" + 0.020*\"tweeted\" + 0.020*\"unnecessarily\"'), (55, '0.529*\"seat\" + 0.100*\"back\" + 0.029*\"hacer\" + 0.029*\"bought\" + 0.014*\"career\" + 0.014*\"chat\" + 0.014*\"part\" + 0.014*\"supervision\" + 0.014*\"added\" + 0.014*\"surprised\"'), (216, '0.085*\"send\" + 0.068*\"unacceptable\" + 0.068*\"find\" + 0.068*\"issuing\" + 0.051*\"forward\" + 0.051*\"thanksgiving\" + 0.051*\"mechanical\" + 0.034*\"inform\" + 0.034*\"flying\" + 0.034*\"caste\"'), (83, '0.259*\"maintenance\" + 0.103*\"tarmac\" + 0.052*\"status\" + 0.034*\"start\" + 0.034*\"outlet\" + 0.034*\"speaking\" + 0.034*\"worse\" + 0.034*\"dropped\" + 0.034*\"france\" + 0.017*\"exceeding\"'), (197, '0.128*\"phone\" + 0.106*\"flight\" + 0.085*\"amazing\" + 0.064*\"minor\" + 0.064*\"comment\" + 0.043*\"good\" + 0.021*\"adding\" + 0.021*\"insult\" + 0.021*\"switch\" + 0.021*\"oppose\"'), (115, '0.294*\"club\" + 0.162*\"admiral\" + 0.059*\"team\" + 0.044*\"claim\" + 0.029*\"guy\" + 0.029*\"tonight\" + 0.029*\"class\" + 0.015*\"causing\" + 0.015*\"cheese\" + 0.015*\"early\"'), (239, '0.122*\"refund\" + 0.122*\"blah\" + 0.073*\"half\" + 0.049*\"company\" + 0.049*\"entire\" + 0.049*\"terminal\" + 0.049*\"reduce\" + 0.049*\"guess\" + 0.049*\"finally\" + 0.024*\"proper\"')]\n"
     ]
    }
   ],
   "source": [
    "# Select the model and print the topics\n",
    "optimal_model = model_list[3]\n",
    "model_topics = optimal_model.show_topics(formatted=False)\n",
    "print(optimal_model.print_topics(num_words=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Attribution des topics pour tous les messages du dataset dans un m√™me dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>give, gate, back, announced, miss, wing, fille...</td>\n",
       "      <td>@AmericanAir Erica on the lax team is amazing ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>make, mile, flight, comfortable, made, asap, b...</td>\n",
       "      <td>@AmericanAir Could you have someone on your la...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>235.0</td>\n",
       "      <td>0.0381</td>\n",
       "      <td>airline, american, week, contact, reconocerlo,...</td>\n",
       "      <td>Ben Tennyson and an American Airlines pilot. üéÉ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>back, flyer, frequent, program, trip, mile, bu...</td>\n",
       "      <td>@AmericanAir Right, but I earned those. I also...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0227</td>\n",
       "      <td>back, attendant, voucher, wanted, checked, mai...</td>\n",
       "      <td>Thank you, @AmericanAir for playing #ThisIsUs ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>74.0</td>\n",
       "      <td>0.0600</td>\n",
       "      <td>wifi, making, delta, dont, loyal, easy, teach,...</td>\n",
       "      <td>@AmericanAir's wifi makes Amtrak's wifi look p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>115.0</td>\n",
       "      <td>0.0582</td>\n",
       "      <td>club, admiral, team, claim, guy, tonight, clas...</td>\n",
       "      <td>Wonderful club! @americanair (@ American Airli...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7</td>\n",
       "      <td>65.0</td>\n",
       "      <td>0.0251</td>\n",
       "      <td>attendant, visit, finally, shit, changed, peop...</td>\n",
       "      <td>@AmericanAir already did...changed browsers, d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>8</td>\n",
       "      <td>69.0</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>hold, return, family, enjoy, browser, reservat...</td>\n",
       "      <td>@AmericanAir ........still....on....hold.....t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>9</td>\n",
       "      <td>34.0</td>\n",
       "      <td>0.0156</td>\n",
       "      <td>ticket, delayed, worst, stop, expectation, buy...</td>\n",
       "      <td>@AmericanAir well now i am told the ticket cos...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0           226.0              0.0159   \n",
       "1            1           232.0              0.0121   \n",
       "2            2           235.0              0.0381   \n",
       "3            3             1.0              0.0139   \n",
       "4            4           192.0              0.0227   \n",
       "5            5            74.0              0.0600   \n",
       "6            6           115.0              0.0582   \n",
       "7            7            65.0              0.0251   \n",
       "8            8            69.0              0.0121   \n",
       "9            9            34.0              0.0156   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  give, gate, back, announced, miss, wing, fille...   \n",
       "1  make, mile, flight, comfortable, made, asap, b...   \n",
       "2  airline, american, week, contact, reconocerlo,...   \n",
       "3  back, flyer, frequent, program, trip, mile, bu...   \n",
       "4  back, attendant, voucher, wanted, checked, mai...   \n",
       "5  wifi, making, delta, dont, loyal, easy, teach,...   \n",
       "6  club, admiral, team, claim, guy, tonight, clas...   \n",
       "7  attendant, visit, finally, shit, changed, peop...   \n",
       "8  hold, return, family, enjoy, browser, reservat...   \n",
       "9  ticket, delayed, worst, stop, expectation, buy...   \n",
       "\n",
       "                                                Text  \n",
       "0  @AmericanAir Erica on the lax team is amazing ...  \n",
       "1  @AmericanAir Could you have someone on your la...  \n",
       "2  Ben Tennyson and an American Airlines pilot. üéÉ...  \n",
       "3  @AmericanAir Right, but I earned those. I also...  \n",
       "4  Thank you, @AmericanAir for playing #ThisIsUs ...  \n",
       "5  @AmericanAir's wifi makes Amtrak's wifi look p...  \n",
       "6  Wonderful club! @americanair (@ American Airli...  \n",
       "7  @AmericanAir already did...changed browsers, d...  \n",
       "8  @AmericanAir ........still....on....hold.....t...  \n",
       "9  @AmericanAir well now i am told the ticket cos...  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def format_topics_sentences(ldamodel=ldamallet, corpus=corpus, texts=text_data):\n",
    "    # Init output\n",
    "    sent_topics_df = pd.DataFrame()\n",
    "\n",
    "    # Get main topic in each document\n",
    "    for i, row in enumerate(ldamodel[corpus]):\n",
    "        row = sorted(row, key=lambda x: (x[1]), reverse=True)\n",
    "        # Get the Dominant topic, Perc Contribution and Keywords for each document\n",
    "        for j, (topic_num, prop_topic) in enumerate(row):\n",
    "            if j == 0:  # => dominant topic\n",
    "                wp = ldamodel.show_topic(topic_num)\n",
    "                topic_keywords = \", \".join([word for word, prop in wp])\n",
    "                sent_topics_df = sent_topics_df.append(pd.Series([int(topic_num), round(prop_topic,4), topic_keywords]), ignore_index=True)\n",
    "            else:\n",
    "                break\n",
    "    sent_topics_df.columns = ['Dominant_Topic', 'Perc_Contribution', 'Topic_Keywords']\n",
    "\n",
    "    # Add original text to the end of the output\n",
    "    contents = pd.Series(texts)\n",
    "    sent_topics_df = pd.concat([sent_topics_df, contents], axis=1)\n",
    "    return(sent_topics_df)\n",
    "\n",
    "\n",
    "df_topic_sents_keywords = format_topics_sentences(ldamodel=optimal_model, corpus=corpus, texts=df.question.values.tolist())\n",
    "\n",
    "# Format\n",
    "df_dominant_topic = df_topic_sents_keywords.reset_index()\n",
    "df_dominant_topic.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Ajout de la colonne 'responce' dans le dataframe r√©alis√© precedemment contenant les questions et leur topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "      <th>responce</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>226.0</td>\n",
       "      <td>0.0159</td>\n",
       "      <td>give, gate, back, announced, miss, wing, fille...</td>\n",
       "      <td>@AmericanAir Erica on the lax team is amazing ...</td>\n",
       "      <td>@115904 We'll be sure to pass along your kind ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>232.0</td>\n",
       "      <td>0.0121</td>\n",
       "      <td>make, mile, flight, comfortable, made, asap, b...</td>\n",
       "      <td>@AmericanAir Could you have someone on your la...</td>\n",
       "      <td>@115904 Our apologies for the delay in respond...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>235.0</td>\n",
       "      <td>0.0381</td>\n",
       "      <td>airline, american, week, contact, reconocerlo,...</td>\n",
       "      <td>Ben Tennyson and an American Airlines pilot. üéÉ...</td>\n",
       "      <td>@115905 Aww, that's definitely a future pilot ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>back, flyer, frequent, program, trip, mile, bu...</td>\n",
       "      <td>@AmericanAir Right, but I earned those. I also...</td>\n",
       "      <td>@115906 We're sorry for your frustration.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>192.0</td>\n",
       "      <td>0.0227</td>\n",
       "      <td>back, attendant, voucher, wanted, checked, mai...</td>\n",
       "      <td>Thank you, @AmericanAir for playing #ThisIsUs ...</td>\n",
       "      <td>@115909 We're glad you got to kick back and en...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0           226.0              0.0159   \n",
       "1            1           232.0              0.0121   \n",
       "2            2           235.0              0.0381   \n",
       "3            3             1.0              0.0139   \n",
       "4            4           192.0              0.0227   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  give, gate, back, announced, miss, wing, fille...   \n",
       "1  make, mile, flight, comfortable, made, asap, b...   \n",
       "2  airline, american, week, contact, reconocerlo,...   \n",
       "3  back, flyer, frequent, program, trip, mile, bu...   \n",
       "4  back, attendant, voucher, wanted, checked, mai...   \n",
       "\n",
       "                                                Text  \\\n",
       "0  @AmericanAir Erica on the lax team is amazing ...   \n",
       "1  @AmericanAir Could you have someone on your la...   \n",
       "2  Ben Tennyson and an American Airlines pilot. üéÉ...   \n",
       "3  @AmericanAir Right, but I earned those. I also...   \n",
       "4  Thank you, @AmericanAir for playing #ThisIsUs ...   \n",
       "\n",
       "                                            responce  \n",
       "0  @115904 We'll be sure to pass along your kind ...  \n",
       "1  @115904 Our apologies for the delay in respond...  \n",
       "2  @115905 Aww, that's definitely a future pilot ...  \n",
       "3          @115906 We're sorry for your frustration.  \n",
       "4  @115909 We're glad you got to kick back and en...  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "complete_df= df_dominant_topic.join(df['responce'], lsuffix='_database', rsuffix='_input')\n",
    "complete_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Input de l'utilisateur et son topic (A faire)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>0.0147</td>\n",
       "      <td>flight, captain, find, send, price, passed, wi...</td>\n",
       "      <td>@AmericanAir Erica on the lax team is amazing ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "0            0           142.0              0.0147   \n",
       "\n",
       "                                            Keywords  \\\n",
       "0  flight, captain, find, send, price, passed, wi...   \n",
       "\n",
       "                                                Text  \n",
       "0  @AmericanAir Erica on the lax team is amazing ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "userinput=preprocess(\"I have passed an agreable moment with you! Thank you!\")\n",
    "dictionaryInput = corpora.Dictionary([userinput])\n",
    "corpusInput = [dictionary.doc2bow(userinput)]\n",
    "\n",
    "df_topic_test = format_topics_sentences(ldamodel=optimal_model, corpus=corpusInput, texts=\"@AmericanAir Erica on the lax team is amazing give her a raise ty\")\n",
    "\n",
    "# Format\n",
    "df_dominant_topic_test = df_topic_test.reset_index()\n",
    "df_dominant_topic_test.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "# Show\n",
    "df_dominant_topic_test.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Filtrage du dataframe selon un topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Document_No</th>\n",
       "      <th>Dominant_Topic</th>\n",
       "      <th>Topic_Perc_Contrib</th>\n",
       "      <th>Keywords</th>\n",
       "      <th>Text</th>\n",
       "      <th>responce</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>546</td>\n",
       "      <td>176.0</td>\n",
       "      <td>0.0139</td>\n",
       "      <td>miami, flying, changing, issue, seat, reward, ...</td>\n",
       "      <td>Does anyone know how we can check to see if ou...</td>\n",
       "      <td>@136438 We're working on this issue and expect...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1843</th>\n",
       "      <td>1843</td>\n",
       "      <td>176.0</td>\n",
       "      <td>0.0162</td>\n",
       "      <td>miami, flying, changing, issue, seat, reward, ...</td>\n",
       "      <td>Without a doubt @AmericanAir has the worst rew...</td>\n",
       "      <td>@172158 We strive for a great rewards program,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Document_No  Dominant_Topic  Topic_Perc_Contrib  \\\n",
       "546           546           176.0              0.0139   \n",
       "1843         1843           176.0              0.0162   \n",
       "\n",
       "                                               Keywords  \\\n",
       "546   miami, flying, changing, issue, seat, reward, ...   \n",
       "1843  miami, flying, changing, issue, seat, reward, ...   \n",
       "\n",
       "                                                   Text  \\\n",
       "546   Does anyone know how we can check to see if ou...   \n",
       "1843  Without a doubt @AmericanAir has the worst rew...   \n",
       "\n",
       "                                               responce  \n",
       "546   @136438 We're working on this issue and expect...  \n",
       "1843  @172158 We strive for a great rewards program,...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "oui = complete_df[complete_df['Dominant_Topic']==176.0]\n",
    "oui.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):    \n",
    "    emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth      \n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "    )\"\"\"\n",
    "    \n",
    "    # Remove stopwords \n",
    "    answers  = []\n",
    "    for i in tweet_data.responce:\n",
    "        i = give_emoji_free_text(i).lower()\n",
    "        answers.append(re.sub(r'\\s+', ' ',\n",
    "                            re.sub(r'\\[[0-9]*\\]', ' ',\n",
    "                            re.sub(r'(@([A-Za-z0-9`~!@#$%^&*()_|+\\-=?;:\\'\",.<>\\{\\}\\[\\]\\\\\\/]{2,32}))', '', # remove tags\n",
    "                            re.sub(r'\\d+', ' ', # remove numbers\n",
    "                            re.sub(r'\\#+', ' ', # remove hashtags\n",
    "                            re.sub(r'http\\S+', ' ', # remove urls\n",
    "                            re.sub(emoticon_string,'', # remove emojis\n",
    "                            re.sub(r'[\\.\\?\\!\\,\\:\\;\\\"]', '', i) # remove punctuation\n",
    "                                  ))))))))\n",
    "    # Lemmatization and tokenization\n",
    "    sentences = []\n",
    "    le = WordNetLemmatizer()\n",
    "    for i in answers:\n",
    "        word_tokens = word_tokenize(i)\n",
    "        lemmas = [ le.lemmatize(w) for w in word_tokens if w not in stop_words and len(w) > 3 ]\n",
    "        cleaned_text=\" \".join(lemmas)\n",
    "        sentences.append(cleaned_text)\n",
    "    \n",
    "\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['working issue expect avoid cancellation', 'strive great reward program hear liking offer']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-eb7aabf9e3c4>:2: DeprecationWarning: 'emoji.get_emoji_regexp()' is deprecated and will be removed in version 2.0.0. If you want to remove emoji from a string, consider the method emoji.replace_emoji(str, replace='').\n",
      "To hide this warning, pin/downgrade the package to 'emoji~=1.6.3'\n",
      "  return emoji.get_emoji_regexp().sub(r'', text)\n"
     ]
    }
   ],
   "source": [
    "tweet_data = oui[[\"Text\",\"responce\"]]\n",
    "responce_data = clean_text(tweet_data.responce) #preprocessing\n",
    "\n",
    "print(responce_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Doc2Vec model\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import preprocess_documents\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_corpus = preprocess_documents(responce_data)\n",
    "tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(responce_data)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[TaggedDocument(words='working issue expect avoid cancellation', tags=[0]),\n",
       " TaggedDocument(words='strive great reward program hear liking offer', tags=[1])]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 1\n",
      "iteration 2\n",
      "iteration 3\n",
      "iteration 4\n",
      "iteration 5\n",
      "iteration 6\n",
      "iteration 7\n",
      "iteration 8\n",
      "iteration 9\n",
      "iteration 10\n",
      "iteration 11\n",
      "iteration 12\n",
      "iteration 13\n",
      "iteration 14\n",
      "iteration 15\n",
      "iteration 16\n",
      "iteration 17\n",
      "iteration 18\n",
      "iteration 19\n",
      "iteration 20\n",
      "iteration 21\n",
      "iteration 22\n",
      "iteration 23\n",
      "iteration 24\n",
      "iteration 25\n",
      "iteration 26\n",
      "iteration 27\n",
      "iteration 28\n",
      "iteration 29\n",
      "iteration 30\n",
      "iteration 31\n",
      "iteration 32\n",
      "iteration 33\n",
      "iteration 34\n",
      "iteration 35\n",
      "iteration 36\n",
      "iteration 37\n",
      "iteration 38\n",
      "iteration 39\n",
      "iteration 40\n",
      "iteration 41\n",
      "iteration 42\n",
      "iteration 43\n",
      "iteration 44\n",
      "iteration 45\n",
      "iteration 46\n",
      "iteration 47\n",
      "iteration 48\n",
      "iteration 49\n",
      "iteration 50\n",
      "iteration 51\n",
      "iteration 52\n",
      "iteration 53\n",
      "iteration 54\n",
      "iteration 55\n",
      "iteration 56\n",
      "iteration 57\n",
      "iteration 58\n",
      "iteration 59\n",
      "iteration 60\n",
      "iteration 61\n",
      "iteration 62\n",
      "iteration 63\n",
      "iteration 64\n",
      "iteration 65\n",
      "iteration 66\n",
      "iteration 67\n",
      "iteration 68\n",
      "iteration 69\n",
      "iteration 70\n",
      "iteration 71\n",
      "iteration 72\n",
      "iteration 73\n",
      "iteration 74\n",
      "iteration 75\n",
      "iteration 76\n",
      "iteration 77\n",
      "iteration 78\n",
      "iteration 79\n",
      "iteration 80\n",
      "iteration 81\n",
      "iteration 82\n",
      "iteration 83\n",
      "iteration 84\n",
      "iteration 85\n",
      "iteration 86\n",
      "iteration 87\n",
      "iteration 88\n",
      "iteration 89\n",
      "iteration 90\n",
      "iteration 91\n",
      "iteration 92\n",
      "iteration 93\n",
      "iteration 94\n",
      "iteration 95\n",
      "iteration 96\n",
      "iteration 97\n",
      "iteration 98\n",
      "iteration 99\n",
      "Model Saved\n"
     ]
    }
   ],
   "source": [
    "model = Doc2Vec(tagged_data, \n",
    "                vector_size=200, \n",
    "                epochs=100, \n",
    "                hs=1, # use of hierachical softmax\n",
    "                min_count=1, \n",
    "                window=2, # Max distance\n",
    "                dm =0) # PV-Bag of words used\n",
    "  \n",
    "#model.build_vocab(tagged_data)\n",
    "train_corpus = [taggeddoc for taggeddoc_list in tagged_data for taggeddoc in taggeddoc_list]\n",
    "\n",
    "for epoch in range(model.epochs):\n",
    "    print('iteration {0}'.format(epoch))\n",
    "    model.train(tagged_data,\n",
    "                total_examples=2,\n",
    "                epochs=100)\n",
    "    # decrease the learning rate\n",
    "    model.alpha -= 0.0002\n",
    "    # fix the learning rate, no decay\n",
    "    model.min_alpha = model.alpha\n",
    "\n",
    "model.save(\"d2v.model\")\n",
    "print(\"Model Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response_sentences(most_similar, cleaned_sentences):\n",
    "    answer = []\n",
    "    for label,index in [('MOST', 0)]:#, ('SECOND-MOST', 1), ('MEDIAN', len(most_similar)//2), ('LEAST', len(most_similar) - 1)]:\n",
    "        answer.append(cleaned_sentences[int(most_similar[index][0])])\n",
    "        #print(u'%s %s: %s\\n' % (label, most_similar[index][1], cleaned_sentences[int(most_similar[index][0])]))\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_intent(user_query):\n",
    "    userinput=preprocess(user_query)\n",
    "    dictionaryInput = corpora.Dictionary([userinput])\n",
    "    corpusInput = [dictionary.doc2bow(userinput)]\n",
    "\n",
    "    df_topic_test = format_topics_sentences(ldamodel=optimal_model, corpus=corpusInput, texts=\"@AmericanAir Erica on the lax team is amazing give her a raise ty\")\n",
    "\n",
    "    # Format\n",
    "    df_dominant_topic_test = df_topic_test.reset_index()\n",
    "    df_dominant_topic_test.columns = ['Document_No', 'Dominant_Topic', 'Topic_Perc_Contrib', 'Keywords', 'Text']\n",
    "\n",
    "    topic_input_value = df_dominant_topic_test['Dominant_Topic'].iloc[0]\n",
    "\n",
    "    df_topic_input = complete_df[complete_df['Dominant_Topic']==topic_input_value]\n",
    "\n",
    "    tweet_data = df_topic_input[[\"Text\",\"responce\"]]\n",
    "    cleaned_sentences = clean_text(tweet_data.responce)\n",
    "    processed_corpus = preprocess_documents(cleaned_sentences)\n",
    "    tagged_data = [TaggedDocument(d, [i]) for i, d in enumerate(cleaned_sentences)]\n",
    "\n",
    "    model = Doc2Vec(tagged_data, \n",
    "                vector_size=200, \n",
    "                epochs=100, \n",
    "                hs=1, # use of hierachical softmax\n",
    "                min_count=1, \n",
    "                window=2, # Max distance\n",
    "                dm =0) # PV-Bag of words used\n",
    "  \n",
    "    #model.build_vocab(tagged_data)\n",
    "    train_corpus = [taggeddoc for taggeddoc_list in tagged_data for taggeddoc in taggeddoc_list]\n",
    "\n",
    "    for epoch in range(model.epochs):\n",
    "        model.train(tagged_data,\n",
    "                total_examples=2,\n",
    "                epochs=100)\n",
    "        # decrease the learning rate\n",
    "        model.alpha -= 0.0002\n",
    "        # fix the learning rate, no decay\n",
    "        model.min_alpha = model.alpha\n",
    "\n",
    "    model.save(\"d2v.model\")\n",
    "\n",
    "    return cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_query):\n",
    "    # Using TFID Add the query to the answers list and vectorize it, then measure the cosine similarity between query and answers\n",
    "    \"\"\"cleaned_sentences.append(user_query)\n",
    "    vectorizer = TfidfVectorizer(stop_words=stop_words,max_features=1000)\n",
    "    sentences_vectors = vectorizer.fit_transform(cleaned_sentences)\n",
    "    \n",
    "    similar_vector_values = cosine_similarity(sentences_vectors[-1], sentences_vectors)\n",
    "    similar_sentence_number = similar_vector_values.argsort()[0][-2]\n",
    "    \n",
    "    matched_vector = similar_vector_values.flatten()\n",
    "    matched_vector.sort()\n",
    "    matched_vector = matched_vector[-2]\n",
    "\n",
    "    if matched_vector == 0: # no vector matching\n",
    "        return \"I didn't understand, can you reformulate please ?\"\n",
    "    else: \n",
    "        return cleaned_sentences[similar_sentence_number]\"\"\"\n",
    "    cleaned_sentences = input_intent(user_query)\n",
    "    \n",
    "    # Using doc2vec\n",
    "    model= Doc2Vec.load(\"d2v.model\")\n",
    "    \n",
    "    # vector of a document which is not in training data\n",
    "    cleaned = word_tokenize(user_query.lower())\n",
    "    v1 = model.infer_vector(cleaned)\n",
    "    #print(\"V1_infer\", v1)\n",
    "\n",
    "    # most similar sentences using tags\n",
    "    similar_doc = model.docvecs.most_similar([v1])\n",
    "    return response_sentences(similar_doc, cleaned_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Greetings\n",
    "greetings = ('hey', 'hello', 'whats up', 'good morning', 'good evening', 'morning', 'evening', 'hello there', 'hey there')\n",
    "greeting_responses = [\"hey\", \"hey hows you?\", \"*nods*\", \"hello, how you doing\", \"hello\", \"Welcome, I am good and you\"]\n",
    "# Goodbye\n",
    "goodbye = ('bye', 'good bye', 'take care')\n",
    "goodbye_responses = ['See you next time !', 'Bye see you next time ! Hope I was helpful :p']\n",
    "# Apology\n",
    "apology = ['sorry', 'I apologize', 'disapointed']\n",
    "# Thanks\n",
    "thanks =('thank you', 'thanks', 'grateful', 'pleased')\n",
    "thanks_responses = ['Always a pleasure to help you !', 'You\\'re welcome !', 'No problem ! :)']\n",
    "def greeting_response(query):\n",
    "    for token in query.split():\n",
    "        if token.lower() in greetings:\n",
    "            return random.choice(greeting_responses)\n",
    "        if token.lower() in thanks:\n",
    "            return random.choice(thanks_responses)\n",
    "        if token.lower() in goodbye:\n",
    "            return random.choice(goodbye_responses)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, I am the bot assistant of American Air, here to answer to your questions. Please ask me anything :))\n",
      "hey\n",
      "Twitter Chatbot: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-eb7aabf9e3c4>:2: DeprecationWarning: 'emoji.get_emoji_regexp()' is deprecated and will be removed in version 2.0.0. If you want to remove emoji from a string, consider the method emoji.replace_emoji(str, replace='').\n",
      "To hide this warning, pin/downgrade the package to 'emoji~=1.6.3'\n",
      "  return emoji.get_emoji_regexp().sub(r'', text)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['strive great reward program hear liking offer']\n",
      "Twitter Chatbot: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-eb7aabf9e3c4>:2: DeprecationWarning: 'emoji.get_emoji_regexp()' is deprecated and will be removed in version 2.0.0. If you want to remove emoji from a string, consider the method emoji.replace_emoji(str, replace='').\n",
      "To hide this warning, pin/downgrade the package to 'emoji~=1.6.3'\n",
      "  return emoji.get_emoji_regexp().sub(r'', text)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-45-f626dda068d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Twitter Chatbot: \"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m             \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m             \u001b[1;31m#cleaned_sentences.remove(query)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-42-4c98bfcd3bce>\u001b[0m in \u001b[0;36mresponse\u001b[1;34m(user_query)\u001b[0m\n\u001b[0;32m     16\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         return cleaned_sentences[similar_sentence_number]\"\"\"\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mcleaned_sentences\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput_intent\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0muser_query\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# Using doc2vec\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-41-2fca29b554f4>\u001b[0m in \u001b[0;36minput_intent\u001b[1;34m(user_query)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         model.train(tagged_data,\n\u001b[0m\u001b[0;32m     34\u001b[0m                 \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m                 epochs=100)\n",
      "\u001b[1;32mc:\\Users\\khauv\\anaconda3\\lib\\site-packages\\gensim\\models\\doc2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, documents, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, callbacks)\u001b[0m\n\u001b[0;32m    808\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'start_doctags'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mstart_doctags\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 810\u001b[1;33m         super(Doc2Vec, self).train(\n\u001b[0m\u001b[0;32m    811\u001b[0m             \u001b[0msentences\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\khauv\\anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, sentences, corpus_file, total_examples, total_words, epochs, start_alpha, end_alpha, word_count, queue_factor, report_delay, compute_loss, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m   1075\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompute_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompute_loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1076\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrunning_training_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0.0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1077\u001b[1;33m         return super(BaseWordEmbeddingsModel, self).train(\n\u001b[0m\u001b[0;32m   1078\u001b[0m             \u001b[0mdata_iterable\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msentences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcorpus_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcorpus_file\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1079\u001b[0m             \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstart_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstart_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend_alpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mend_alpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword_count\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mword_count\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\khauv\\anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self, data_iterable, corpus_file, epochs, total_examples, total_words, queue_factor, report_delay, callbacks, **kwargs)\u001b[0m\n\u001b[0;32m    549\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    550\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mdata_iterable\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 551\u001b[1;33m                 trained_word_count_epoch, raw_word_count_epoch, job_tally_epoch = self._train_epoch(\n\u001b[0m\u001b[0;32m    552\u001b[0m                     \u001b[0mdata_iterable\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    553\u001b[0m                     total_words=total_words, queue_factor=queue_factor, report_delay=report_delay)\n",
      "\u001b[1;32mc:\\Users\\khauv\\anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_train_epoch\u001b[1;34m(self, data_iterable, cur_epoch, total_examples, total_words, queue_factor, report_delay)\u001b[0m\n\u001b[0;32m    485\u001b[0m             \u001b[0mthread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m         trained_word_count, raw_word_count, job_tally = self._log_epoch_progress(\n\u001b[0m\u001b[0;32m    488\u001b[0m             \u001b[0mprogress_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mjob_queue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcur_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcur_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_examples\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_examples\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_words\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m             report_delay=report_delay, is_corpus_file_mode=False)\n",
      "\u001b[1;32mc:\\Users\\khauv\\anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36m_log_epoch_progress\u001b[1;34m(self, progress_queue, job_queue, cur_epoch, total_examples, total_words, report_delay, is_corpus_file_mode)\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 346\u001b[1;33m             \u001b[0mreport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprogress_queue\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# blocks if workers too slow\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mreport\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# a thread reporting that it finished\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m                 \u001b[0munfinished_worker_count\u001b[0m \u001b[1;33m-=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\khauv\\anaconda3\\lib\\queue.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, block, timeout)\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[1;32mwhile\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_qsize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnot_empty\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[1;32melif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"'timeout' must be a non-negative number\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\khauv\\anaconda3\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "print(\"Hello, I am the bot assistant of American Air, here to answer to your questions. Please ask me anything :))\")\n",
    "\n",
    "while(True):\n",
    "    query = input().lower()\n",
    "    if query not in goodbye:\n",
    "        if query in greetings or query in thanks or query in goodbye:\n",
    "            print(greeting_response(query))\n",
    "        else:\n",
    "            print(\"Twitter Chatbot: \", end=\"\")\n",
    "            print(response(query))\n",
    "            #cleaned_sentences.remove(query)\n",
    "    else:\n",
    "        print()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a67b01ff655fedeafbec0f5c154c73501f94f5bc6352173beba5e66e9a24b9e5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
